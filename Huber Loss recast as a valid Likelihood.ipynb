{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sympy as sm\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recast the Huber loss as a valid likelihood function to be used in likelihood ratio tests, we need to derive the associated normalisation constant. We can achieve this using sympy, a symbolic computation package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same notation as in the PyTorchDIA paper, we can start by defining the class of distributions,\n",
    "\n",
    "$f(x) = Q \\times \\frac{1}{\\sigma} \\times \\exp({-\\rho(x)})$.\n",
    "\n",
    "$Q$ is the normalisation constant we're interested in deriving. In order to find $Q$, we need to integrate everything to its right over the entire range of $x$. Setting $Q$ equal to the reciprocal of the result of this integration will ensure that when $f(x)$ is integrated over the entire range of $x$ the result will always be $1$ i.e. $f(x)$ is now a valid probability distribution.\n",
    "\n",
    "We'll start with the Gaussian case, where\n",
    "\n",
    "$\\rho(x) = \\frac{1}{2}\\frac{x^2}{\\sigma^2}\\;.$\n",
    "\n",
    "N.B. Under the above definition for $f(x)$, we already have the $\\sigma$ term that turns up in the usual derivation of the normalisation constant for a Gaussian, so integrating this expression should give us $\\sqrt{2\\pi}$. Let's use sympy to do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sqrt{2} \\sqrt{\\pi}$"
      ],
      "text/plain": [
       "sqrt(2)*sqrt(pi)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, test this out for the gaussian case\n",
    "x, sigma = sm.symbols(\"x, sigma\", real=True, positive=True)\n",
    "rho = ((x / sigma) ** 2 )/ 2\n",
    "loss = sm.exp(-rho) / sigma\n",
    "norm = 2*sm.integrate(loss, (x, 0, sm.oo))\n",
    "sm.simplify(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK great, that seems to work! Now let's do the same for the Huber loss.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\rho_{\\text{Huber}, c}(x) =\n",
    "    \\begin{cases}\n",
    "    \\frac{1}{2}\\frac{x^2}{\\sigma^2}, & \\text{for } |\\frac{x}{\\sigma}|\\leq c \\\\\n",
    "    c(|\\frac{x}{\\sigma}| - \\frac{1}{2}c), & \\text{for } |\\frac{x}{\\sigma}| > c \\;.\n",
    "    \\end{cases}\n",
    "    \\label{eq:huber_loss}\n",
    "\\end{equation}\n",
    "\n",
    "I don't expect the result will be quite as tidy as above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sqrt{2} \\sqrt{\\pi} \\operatorname{erf}{\\left(\\frac{\\sqrt{2} c}{2} \\right)} + \\frac{2 e^{- \\frac{c^{2}}{2}}}{c}$"
      ],
      "text/plain": [
       "sqrt(2)*sqrt(pi)*erf(sqrt(2)*c/2) + 2*exp(-c**2/2)/c"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalisation constant for Huber 'likelihood'\n",
    "x, c, sigma = sm.symbols(\"x, c, sigma\", real=True, positive=True)\n",
    "rho = sm.Piecewise(\n",
    "    ((x / sigma) ** 2 / 2, (x / sigma) <= c),\n",
    "    (c * (sm.Abs(x / sigma) - c / 2), ((x / sigma) > c))\n",
    ")\n",
    "loss = sm.exp(-rho) / sigma\n",
    "norm = 2 * sm.integrate(loss, (x, 0, sm.oo))\n",
    "sm.simplify(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#const = (np.sqrt(2 * np.pi) * math.erf(c / np.sqrt(2))) + ((2 / c) * np.exp(-0.5 * c**2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, on to the meat of this notebook. I'm not certain that it makes sense to compare a Huber likelihood against a Gaussian, even for instances in which there are no outlying data points. We can probe this question with a toy example; fitting a straight line to data with known Gaussian uncertainties.\n",
    "\n",
    "It may be that the Huber likelihood (evaluated at the MLE) is strongly dependent on the tuning paramter, $c$. In the case where $c$ tends to infinity, we should expect the same value for the likelihood as for the Gaussian case. In this special case, not only are all residuals from the model treated as 'inliers', but note too that the normalisation constant we found above would also tend to $\\sqrt{2\\pi}$; the error function in the first term becomes unity and the latter term becomes neglibily small. However, for useful, smaller values of $c$ e.g. 1.345, in which 'outlying' residuals are treated linearly rather than quadratically, should we expect to get roughly the same values for the likelihoods? In other words, does the linear treatment of outliers balance with the change to the normalisation constant, which is itself dependent on $c$. I don't see how it could... but's let's verify this numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd565714a90>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generate some mock data\n",
    "\n",
    "# The linear model with slope 2 and intercept 1:\n",
    "true_params = [2.0, 1.0]\n",
    "\n",
    "# points drawn from true model\n",
    "np.random.seed(42)\n",
    "x = np.sort(np.random.uniform(-2, 2, 30))\n",
    "yerr = 0.4 * np.ones_like(x)\n",
    "y = true_params[0] * x + true_params[1] + yerr * np.random.randn(len(x))\n",
    "\n",
    "# true line\n",
    "x0 = np.linspace(-2.1, 2.1, 200)\n",
    "y0 = np.dot(np.vander(x0, 2), true_params)\n",
    "\n",
    "# plot\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\",k\", ms=0, capsize=0, lw=1, zorder=999)\n",
    "plt.scatter(x, y, marker=\"s\", s=22, c=\"k\", zorder=1000)\n",
    "plt.plot(x0, y0, c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytic solution - MLE for a priori known Gaussian noise\n",
    "def linear_regression(x, y, yerr):\n",
    "    A = np.vander(x, 2)\n",
    "    result = np.linalg.solve(np.dot(A.T, A / yerr[:, None]**2), np.dot(A.T, y / yerr**2))\n",
    "    return result\n",
    "\n",
    "res = linear_regression(x, y, yerr)\n",
    "m, b = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimise Huber loss\n",
    "\n",
    "import torch\n",
    "# initialise model parameters (y = m*x + b)\n",
    "m_robust = torch.nn.Parameter(1e-3*torch.ones(1), requires_grad = True)\n",
    "b_robust = torch.nn.Parameter(1e-3*torch.ones(1), requires_grad = True)\n",
    "params_robust = list([m_robust, b_robust])\n",
    "\n",
    "# negative log-likelihood for Huber likelihood (excluding the irrelevant normalisation constant)\n",
    "def nll_Huber(model, data, yerr, c):\n",
    "    \n",
    "    # ln_sigma is same as above\n",
    "    ln_sigma = torch.log(yerr).sum()\n",
    "\n",
    "    # define inliers and outliers with a threshold, c\n",
    "    resid = torch.abs((model - data)/yerr)\n",
    "    cond1 = resid <= c\n",
    "    cond2 = resid > c\n",
    "    \n",
    "    inliers = ((model - data)/yerr)[cond1]\n",
    "    outliers = ((model - data)/yerr)[cond2]\n",
    "    \n",
    "    # Huber loss can be thought of as a hybrid of l2 and l1 loss\n",
    "    # apply l2 (i.e. normal) loss to inliers, and l1 to outliers\n",
    "    l2 = 0.5*torch.pow(inliers, 2).sum()\n",
    "    l1 = (c * torch.abs(outliers) - (0.5 * c**2)).sum()\n",
    "    \n",
    "    nll = ln_sigma + l2 + l1\n",
    "    \n",
    "    return nll\n",
    "\n",
    "# pass paramterers to optimizer\n",
    "optimizer_robust = torch.optim.Adam(params_robust, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   : loss=-16.117150  m=2.061463  b=0.942204\n",
      "100 : loss=-16.138877  m=2.075715  b=0.943876\n",
      "200 : loss=-16.138879  m=2.075600  b=0.943839\n",
      "300 : loss=-16.138879  m=2.075600  b=0.943839\n",
      "400 : loss=-16.138879  m=2.075600  b=0.943839\n",
      "500 : loss=-16.138879  m=2.075600  b=0.943839\n",
      "600 : loss=-16.138879  m=2.075600  b=0.943839\n",
      "700 : loss=-16.138879  m=2.075600  b=0.943839\n",
      "800 : loss=-16.138879  m=2.075600  b=0.943839\n",
      "900 : loss=-16.138879  m=2.075600  b=0.943839\n",
      "1000: loss=-16.138879  m=2.075600  b=0.943839\n",
      "1100: loss=-16.138879  m=2.075600  b=0.943839\n",
      "1200: loss=-16.138879  m=2.075600  b=0.943839\n",
      "1300: loss=-16.138879  m=2.075600  b=0.943839\n",
      "1400: loss=-16.138879  m=2.075600  b=0.943839\n",
      "1500: loss=-16.138879  m=2.075600  b=0.943839\n",
      "1600: loss=-16.138879  m=2.075600  b=0.943839\n",
      "1700: loss=-16.138879  m=2.075600  b=0.943839\n",
      "1800: loss=-16.138879  m=2.075601  b=0.943839\n",
      "1900: loss=-16.138879  m=2.075601  b=0.943839\n"
     ]
    }
   ],
   "source": [
    "# tuning parameter, c\n",
    "c = 1.345\n",
    "\n",
    "xt, yt, yerrt = torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(yerr)\n",
    "\n",
    "for epoch in range(2000): \n",
    "\n",
    "    model = m_robust*xt + b_robust\n",
    "    \n",
    "    # negative loglikelihood\n",
    "    loss = nll_Huber(model, yt, yerrt, c)\n",
    "    \n",
    "    optimizer_robust.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer_robust.step()\n",
    "    \n",
    "    if np.mod(epoch, 100) == 0:\n",
    "        # You can see the alpha+scale parameters moving around.\n",
    "        print('{:<4}: loss={:03f}  m={:03f}  b={:03f}'.format(\n",
    "            epoch, loss.item(), m_robust.item(), b_robust.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_r, b_r = m_robust.item(), b_robust.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gaussian_log_likelihood(data, model, var):\n",
    "    print('\\nGaussian log-likelihood')\n",
    "    chi2 = 0.5 * ((data - model)**2 / var).sum()\n",
    "    lnsigma = np.log(np.sqrt(var)).sum()\n",
    "    norm_constant = len(data.flatten()) * 0.5 * np.log(2 * np.pi)\n",
    "    print('chi2, lnsigma, norm_constant:', chi2, lnsigma, norm_constant)\n",
    "    return -(chi2 + lnsigma + norm_constant)\n",
    "\n",
    "def evaluate_huber_log_likelihood(data, model, var, c):\n",
    "    print('\\nHuber log-likelihood')\n",
    "    \n",
    "    ## PyTorchDIA - 'Huber' likelihood\n",
    "    sigma = np.sqrt(var)\n",
    "    ln_sigma = np.log(sigma).sum()\n",
    "\n",
    "    # gaussian when (model - targ)/sigma <= c\n",
    "    # absolute deviation when (model - targ)/sigma > c\n",
    "    cond1 = np.abs((model - data)/sigma) <= c\n",
    "    cond2 = np.abs((model - data)/sigma) > c\n",
    "    inliers = ((model - data)/sigma)[cond1]\n",
    "    outliers = ((model - data)/sigma)[cond2]\n",
    "\n",
    "    l2 = 0.5*(np.power(inliers, 2)).sum()\n",
    "    l1 = (c *(np.abs(outliers)) - (0.5 * c**2)).sum()\n",
    "\n",
    "    constant = (np.sqrt(2 * np.pi) * math.erf(c / np.sqrt(2))) + ((2 / c) * np.exp(-0.5 * c**2)) \n",
    "    norm_constant = len(data.flatten()) * np.log(constant)\n",
    "    ll = -(l2 + l1 + ln_sigma + norm_constant)\n",
    "    print('l2, l1, ln_sigma, norm_constant:', l2, l1, ln_sigma, norm_constant)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE models\n",
    "model = m*x + b # Gaussian\n",
    "model_r = m_r*x + b_r # Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian log-likelihood\n",
      "chi2, lnsigma, norm_constant: 173.83346014331022 -27.488721956224655 27.56815599614018\n",
      "-173.91289418322575\n",
      "\n",
      "Huber log-likelihood\n",
      "l2, l1, ln_sigma, norm_constant: 4.538741913408627 81.53652455056039 -27.488721956224655 29.357945837155103\n",
      "-87.94449034489946\n"
     ]
    }
   ],
   "source": [
    "ll_gaussian = evaluate_gaussian_log_likelihood(x, model, yerr**2)\n",
    "print(ll_gaussian)\n",
    "ll_huber = evaluate_huber_log_likelihood(x, model_r, yerr**2, c=c)\n",
    "print(ll_huber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless I've blundered, it seems like the Huber log-likelihood is always going to exceed the Gaussian due to how the numerics are treated. If this is indeed correct, then any comparison between the two would be meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
