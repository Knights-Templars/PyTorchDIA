{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the simulated image tests in Section 3 of the manuscript. It generates pairs of images with additive gaussian noise as outlined in Section 3.1, and uses both the Bramich 2008 (B08) and PyTorchDIA algorithms to infer the associated kernel and (scalar) differential background. For each difference image pair, we compute the model fit quality and photometric accuracy metrics in Section 3.2, and append each result to separate text files for the B08 and PyTorchDIA solutions respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we'll run a couple of cells required for the B08 algorithm. These are taken from the pyDANDIA pipeline, https://github.com/pyDANDIA/pyDANDIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "DTYPE = np.float64\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "\n",
    "# compile suggestion: gcc -shared -pthread -fPIC -fwrapv -O2 -Wall -fno-strict-aliasing -I/somepath/include/python2.7 -o umatrix_routine.so umatrix_routine.c\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking\n",
    "@cython.wraparound(False)  # turn off negative index wrapping\n",
    "@cython.nonecheck(False)  # turn off negative index wrapping\n",
    "\n",
    "def umatrix_construction(np.ndarray[DTYPE_t, ndim = 2] reference_image,np.ndarray[DTYPE_t, ndim = 2] weights, pandq, n_kernel_np, kernel_size_np):\n",
    "\n",
    "    cdef int ni_image = np.shape(reference_image)[0]\n",
    "    cdef int nj_image = np.shape(reference_image)[1]\n",
    "    cdef double sum_acc = 0.\n",
    "    cdef int idx_l,idx_m,idx_l_prime,idx_m_prime,idx_i,idx_j\n",
    "    cdef int kernel_size = np.int(kernel_size_np)\n",
    "    cdef int kernel_size_half = np.int(kernel_size_np)/2\n",
    "    cdef int n_kernel = np.int(n_kernel_np)\n",
    "    cdef np.ndarray u_matrix = np.zeros([n_kernel + 1, n_kernel + 1], dtype=DTYPE)\n",
    "\n",
    "    for idx_p in range(n_kernel):\n",
    "        for idx_q in range(idx_p,n_kernel):\n",
    "            sum_acc = 0.\n",
    "            idx_l, idx_m = pandq[idx_p]\n",
    "            idx_l_prime, idx_m_prime = pandq[idx_q]\n",
    "            for idx_i in range(kernel_size_half,ni_image-kernel_size+kernel_size_half+1):\n",
    "                for idx_j in range(kernel_size_half,nj_image-kernel_size+kernel_size_half+1):\n",
    "                    sum_acc += reference_image[idx_i + idx_l, idx_j + idx_m] * reference_image[idx_i + idx_l_prime,idx_j + idx_m_prime]  * weights[idx_i, idx_j]\n",
    "            u_matrix[idx_p, idx_q] = sum_acc\n",
    "            u_matrix[idx_q, idx_p] = sum_acc\n",
    "\n",
    "    for idx_p in [n_kernel]:\n",
    "        for idx_q in range(n_kernel):\n",
    "            sum_acc = 0.\n",
    "            idx_l = kernel_size\n",
    "            idx_m = kernel_size\n",
    "            idx_l_prime, idx_m_prime = pandq[idx_q]\n",
    "            for idx_i in range(kernel_size_half,ni_image-kernel_size+kernel_size_half+1):\n",
    "                for idx_j in range(kernel_size_half,nj_image-kernel_size+kernel_size_half+1):\n",
    "                    sum_acc += reference_image[idx_i + idx_l_prime, idx_j + idx_m_prime] * weights[idx_i, idx_j]\n",
    "            u_matrix[idx_p, idx_q] = sum_acc\n",
    "    \n",
    "    for idx_p in range(n_kernel):\n",
    "        for idx_q in [n_kernel]:\n",
    "            sum_acc = 0.\n",
    "            idx_l, idx_m = pandq[idx_p]\n",
    "            idx_l_prime = kernel_size\n",
    "            idl_m_prime = kernel_size\n",
    "            for idx_i in range(kernel_size_half,ni_image-kernel_size+kernel_size_half+1):\n",
    "                for idx_j in range(kernel_size_half, nj_image-kernel_size+kernel_size_half+1):\n",
    "                    sum_acc += reference_image[idx_i + idx_l, idx_j + idx_m] * weights[idx_i, idx_j] \n",
    "            u_matrix[idx_p, idx_q] = sum_acc\n",
    "\n",
    "    sum_acc = 0.\n",
    "    for idx_i in range(ni_image):\n",
    "        for idx_j in range(nj_image):\n",
    "            sum_acc += weights[idx_i, idx_j] \n",
    "    u_matrix[n_kernel, n_kernel] = sum_acc\n",
    "    \n",
    "    return u_matrix\n",
    "\n",
    "def bvector_construction(np.ndarray[DTYPE_t, ndim = 2] reference_image,np.ndarray[DTYPE_t, ndim = 2] data_image,np.ndarray[DTYPE_t, ndim = 2] weights, pandq, n_kernel_np, kernel_size_np):\n",
    "\n",
    "    cdef int ni_image = np.shape(data_image)[0]\n",
    "    cdef int nj_image = np.shape(data_image)[1]\n",
    "    cdef double sum_acc = 0.\n",
    "    cdef int idx_l,idx_m,idx_l_prime,idx_m_prime,idx_i,idx_j\n",
    "    cdef int kernel_size = np.int(kernel_size_np)\n",
    "    cdef int kernel_size_half = np.int(kernel_size_np)/2\n",
    "    cdef int n_kernel = np.int(n_kernel_np)\n",
    "        \n",
    "    cdef np.ndarray b_vector = np.zeros([n_kernel + 1], dtype=DTYPE)\n",
    "    for idx_p in range(n_kernel):\n",
    "        idx_l, idx_m = pandq[idx_p]\n",
    "        sum_acc = 0.\n",
    "        for idx_i in range(kernel_size_half,ni_image-kernel_size+kernel_size_half+1):\n",
    "            for idx_j in range(kernel_size_half,nj_image-kernel_size+kernel_size_half+1):\n",
    "                   sum_acc += data_image[idx_i, idx_j] * reference_image[idx_i + idx_l , idx_j + idx_m ] * weights[idx_i, idx_j]\n",
    "        b_vector[idx_p] = sum_acc\n",
    "\n",
    "    sum_acc = 0.\n",
    "    for idx_i in range(ni_image):\n",
    "        for idx_j in range(nj_image):\n",
    "            sum_acc += data_image[idx_i, idx_j] * weights[idx_i, idx_j]\n",
    "    b_vector[n_kernel] = sum_acc\n",
    "\n",
    "    return b_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other useful imports\n",
    "import os\n",
    "import astropy\n",
    "from astropy.io import fits\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from MakeFakeImage import MakeFake # a custom script to generate the images\n",
    "import time\n",
    "import torch # PyTorch\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import PyTorchDIA_CCD # PyTorchDIA implementation with a gaussian, CCD noise model\n",
    "torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify all the functions required ##\n",
    "## the B08 specific functions are once again adapted from pyDANDIA\n",
    "\n",
    "# required for B08 approach to deal with edge pixels (convolutions don't like edges!)\n",
    "def extend_image(image, kernel_size):\n",
    "    image_extended = np.zeros((np.shape(image)[0] + 2 * kernel_size,\n",
    "                             np.shape(image)[1] + 2 * kernel_size))\n",
    "    image_extended[kernel_size:-kernel_size, kernel_size:-kernel_size] = np.array(image, float)\n",
    "    \n",
    "    return image_extended\n",
    "\n",
    "\n",
    "def extend_image_hw(image, kernel_size):\n",
    "    image_extended = np.zeros((np.shape(image)[0] + kernel_size - 1,\n",
    "                             np.shape(image)[1] + kernel_size - 1))\n",
    "    hwidth = np.int((kernel_size - 1) / 2)\n",
    "    image_extended[hwidth:image_extended.shape[0]-hwidth,\n",
    "                   hwidth:image_extended.shape[1]-hwidth] = np.array(image, float)\n",
    "    return image_extended\n",
    "\n",
    "# add gaussian noise to image under the standard CCD noise model\n",
    "# N.B. Gain and flat-field are equal to 1, so we only include\n",
    "# the readout noise [ADU] and the photon shot noise (in the gaussian limit)\n",
    "def add_noise_to_image(image, read_noise):\n",
    "    noise_map = np.random.normal(0, 1, size=image.shape)\n",
    "    sigma_imag = np.sqrt(read_noise**2 + image)\n",
    "    image += noise_map*sigma_imag\n",
    "    return image, sigma_imag\n",
    "\n",
    "# adds 10 times **less** variance than add_noise_to_image\n",
    "def add_less_noise_to_image(image, read_noise):\n",
    "    noise_map = np.random.normal(0, 1, size=image.shape)\n",
    "    sigma_imag = 10**(-0.5) * np.sqrt(read_noise**2 + image)\n",
    "    image += noise_map*sigma_imag\n",
    "    return image, sigma_imag\n",
    "\n",
    "# function to build the kernel, U matrix and b vector\n",
    "def construct_kernel_and_matrices(kernel_size, R, I, weights):\n",
    "\n",
    "    pandq = []\n",
    "    n_kernel = kernel_size * kernel_size\n",
    "    ncount = 0\n",
    "    half_kernel_size = int(int(kernel_size) / 2)\n",
    "    for lidx in range(kernel_size):\n",
    "        for midx in range(kernel_size):\n",
    "            pandq.append((lidx - half_kernel_size, midx - half_kernel_size))\n",
    "\n",
    "\n",
    "    R = R.astype('float64')\n",
    "    I =  I.astype('float64')\n",
    "    weights = weights.astype('float64')\n",
    "\n",
    "    start_time = time.time()\n",
    "    U = umatrix_construction(R, weights, pandq, n_kernel, kernel_size)\n",
    "    b = bvector_construction(R, I, weights, pandq, n_kernel, kernel_size)\n",
    "    print(\"--- Finished U and b construction in %s seconds ---\" % (time.time() - start_time))\n",
    "    return U, b\n",
    "\n",
    "\n",
    "# returns the ML least-squares solution for the B08 approach\n",
    "def lstsq_solution(R, I, U, b, kernel_size):\n",
    "    \n",
    "    lstsq_result = np.linalg.lstsq(np.array(U), np.array(b), rcond=None)\n",
    "    a_vector = lstsq_result[0]\n",
    "    lstsq_fit = np.dot(np.array(U), a_vector)\n",
    "    resid = np.array(b) - lstsq_fit\n",
    "    reduced_chisqr = np.sum(resid ** 2) / (float(kernel_size * kernel_size))\n",
    "    lstsq_cov = np.dot(np.array(U).T, np.array(U)) * reduced_chisqr\n",
    "    resivar = np.var(resid, ddof=0) * float(len(a_vector))\n",
    "    \n",
    "    # use pinv in order to stabilize calculation\n",
    "    a_var = np.diag(np.linalg.pinv(lstsq_cov) * resivar)\n",
    "\n",
    "    a_vector_err = np.sqrt(a_var)\n",
    "    output_kernel = np.zeros(kernel_size * kernel_size, dtype=float)\n",
    "    if len(a_vector) > kernel_size * kernel_size:\n",
    "        output_kernel = a_vector[:-1]\n",
    "    else:\n",
    "        output_kernel = a_vector\n",
    "    output_kernel = output_kernel.reshape((kernel_size, kernel_size))\n",
    "\n",
    "    err_kernel = np.zeros(kernel_size * kernel_size, dtype=float)\n",
    "    if len(a_vector) > kernel_size * kernel_size:\n",
    "        err_kernel = a_vector_err[:-1]\n",
    "        err_kernel = err_kernel.reshape((kernel_size, kernel_size))\n",
    "    else:\n",
    "        err_kernel = a_vector_err\n",
    "        err_kernel = err_kernel.reshape((kernel_size, kernel_size))\n",
    "\n",
    "    output_kernel_2 = np.flip(np.flip(output_kernel, 0), 1)\n",
    "    err_kernel_2 = np.flip(np.flip(err_kernel, 0), 1)\n",
    "    bkg_kernel = a_vector[-1]\n",
    "    output_kernel_2.shape\n",
    "\n",
    "    return output_kernel_2, bkg_kernel\n",
    "\n",
    "# simply returns the model image\n",
    "def model_image(R, kernel, B0):\n",
    "    model = convolve2d(R, kernel, mode='same') + B0\n",
    "    return model\n",
    "\n",
    "# return the mean-squared-error (MSE) fit quality metric\n",
    "def calc_MSE(M, I_noiseless, kernel_size):\n",
    "    N_data = len(I_noiseless.flatten())\n",
    "    MSE = 1./(N_data) * np.sum((M - I_noiseless)**2)\n",
    "    return MSE\n",
    "\n",
    "# returns the mean-fit-bias (MFB) and mean-fit-variance (MFV) fit quality metrics\n",
    "def calc_MFB_and_MFV(M, I, noise_map, kernel_size):\n",
    "    N_data = len(I.flatten())\n",
    "    MFB = 1./(N_data) * np.sum((I - M)/noise_map)\n",
    "    MFV = 1./(N_data - 1) * np.sum((((I - M)/noise_map) - MFB)**2)\n",
    "    return MFB, MFV\n",
    "\n",
    "# plots the normalised residuals (epsilon) in D overlain with a unit gaussian\n",
    "def plot_normalised_residuals(epsilon):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.hist(epsilon.flatten(), bins='auto', density=True)\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    plt.plot(x, norm.pdf(x, 0, 1))\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xlabel('Normalised residuals', fontsize=20)\n",
    "    plt.ylabel('Probability', fontsize=20)\n",
    "    plt.show();\n",
    "\n",
    "# plot a 2D numpy array    \n",
    "def plot_image(image, title):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(image)\n",
    "    plt.colorbar()\n",
    "    plt.show();\n",
    "\n",
    "# returns the normalised_psf_object for the PSF fitting photometry\n",
    "def normalised_psf_object(psf_sigma, psf_size, shifts):\n",
    "    # first of, let's create the normalised PSF object to fit\n",
    "    psf = np.zeros((psf_size, psf_size))\n",
    "    nx, ny = psf_size, psf_size\n",
    "    xg, yg = np.meshgrid(range(nx), range(ny))\n",
    "    ## Let's extend this to 2 dimensions ##\n",
    "    centre = np.int(nx/2)\n",
    "    #pos = [centre, centre]\n",
    "    pos = [centre + shifts[0], centre + shifts[1]]\n",
    "    kernel = np.exp(-0.5 * ((xg - pos[0]) ** 2 + (yg - pos[1]) ** 2)/ psf_sigma ** 2)\n",
    "    kernel /= np.sum(kernel) # normalise\n",
    "    psf += kernel #/ (2. * np.pi * psf_sigma ** 2)\n",
    "    return psf\n",
    "\n",
    "# cutout around the central, brightest star\n",
    "def cutout(image, c_size):\n",
    "    centre = np.int(image.shape[0]/2)\n",
    "    radius = np.int((c_size/2))\n",
    "    cutout = image[centre - radius:centre + radius + 1, centre - radius:centre + radius + 1]\n",
    "    \n",
    "    # Generate circular mask\n",
    "    mask_stamp = np.random.normal(0, 3, size=(c_size, c_size))\n",
    "    for row in mask_stamp:\n",
    "        for pixel in row:\n",
    "            coords = np.where(mask_stamp == pixel)\n",
    "            y_coord, x_coord = coords[0][0], coords[1][0]\n",
    "            centre_x, centre_y = np.int(cutout.shape[0]/2), np.int(cutout.shape[0]/2)\n",
    "            delta_x = x_coord - centre_x\n",
    "            delta_y = y_coord - centre_y\n",
    "            distance = np.sqrt(delta_x**2 + delta_y**2)\n",
    "            if distance < radius-1:\n",
    "                mask_stamp[coords] = 0\n",
    "            else:\n",
    "                mask_stamp[coords] = 1\n",
    "    \n",
    "    # mask cutout\n",
    "    #cutout = np.ma.array(cutout, mask=mask_stamp)\n",
    "    \n",
    "    return cutout\n",
    "\n",
    "# results the PSF fitting photometry\n",
    "def fit_results_and_resids(fit, psf, data):\n",
    "    F_pred, const_pred = fit.x[0], fit.x[1]\n",
    "    covmatrix = fit.hess_inv\n",
    "    var = np.diag(covmatrix)\n",
    "    print('\\nPSF fit results:')\n",
    "    print('F_pred:', F_pred, np.sqrt(var[0]))\n",
    "    print('Additive constant:', const_pred, np.sqrt(var[1]))\n",
    "    message = fit.message\n",
    "    return F_pred, const_pred, message\n",
    "\n",
    "# save a np.ndarray as .fits\n",
    "def save_numpy_as_fits(numpy_array, filename):\n",
    "    hdu = fits.PrimaryHDU(numpy_array)\n",
    "    hdul = fits.HDUList([hdu])\n",
    "    hdul.writeto(filename, overwrite=True)\n",
    "    \n",
    "# compute log-likelihood of data given the (MLE) model\n",
    "def evaluate_log_likelihood(data, model, var):\n",
    "    chi2 = (data - model)**2 / var\n",
    "    ln_sigma = np.log(var)\n",
    "    norm_constant = (len(data.flatten()) / 2) * np.log(2 * np.pi)\n",
    "    return -(0.5*chi2.sum() + ln_sigma.sum() + norm_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Simulation: 0\n",
      "Generating reference...\n",
      "Max flux: 13670.299078749502\n",
      "Frac for 142x142 image: 0.3353160435001864\n",
      "Reference properties\n",
      "Reference size: 142\n",
      "Number of sources: 17\n",
      "PSF standard deviation: 2.296175704929089\n",
      "Sky level: 915.288635324258\n",
      "F_max/F_total: 0.3353160435001864\n",
      "Target kernel properties:\n",
      "Kernel standard deviation: 0.5064648501726285\n",
      "Kernel size: 19\n",
      "shift_x, shift_y: [0.45491893] [-0.10441395]\n",
      "Max flux: 13670.299078749502\n",
      "Frac for 142x142 image: 0.3353160435001864\n",
      "Reference and target image shapes:\n",
      "(142, 142) (142, 142)\n",
      "Reference SNR: 29.575924498442177\n",
      "Target SNR: 9.35272853202504\n",
      "Sky subtracting reference.\n",
      "\n",
      "B08 solution, iteration 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-70e8a3aaa89d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mext_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_image_hw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_kernel_and_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_imag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstsq_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_imag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6ed68b191b37>\u001b[0m in \u001b[0;36mconstruct_kernel_and_matrices\u001b[0;34m(kernel_size, R, I, weights)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumatrix_construction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbvector_construction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Finished U and b construction in %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# number of simulations to run\n",
    "n_simulations = 50000\n",
    "\n",
    "# number of iterations to perform for each simulation\n",
    "# for the first three iterations, we update the noise model\n",
    "# used by the pyDANDIA solution. On the last i.e. 4th iteration\n",
    "# we call the PyTorch code, which iterates 3 times automatically\n",
    "n_iters = 4 # 4 normally!\n",
    "\n",
    "for simulation in range(0, n_simulations):\n",
    "    \n",
    "    #np.random.seed(42)\n",
    "    \n",
    "    pyDANDIA, PyTorch = False, False\n",
    "    print('\\n\\nSimulation:', simulation)\n",
    "    # generate noiseless reference 'ref'\n",
    "    print('Generating reference...')\n",
    "    size = 142\n",
    "    log_density = np.random.uniform(0, 3, 1)[0]\n",
    "    star_density = 10**log_density # stars per 100x100 pixels\n",
    "    n_sources = np.int(star_density * (size**2/100**2))       \n",
    "    phi_r = np.random.uniform(0.5, 2.5, 1)[0] # in pixels, this is ~ [1 - 6] fwhm\n",
    "    sky = np.random.uniform(10, 1000, 1)[0] # ADU\n",
    "    \n",
    "    # positions\n",
    "    positions_x = np.random.uniform(0, size, (n_sources,1))\n",
    "    positions_y = np.random.uniform(0, size, (n_sources,1))\n",
    "    positions = np.hstack((positions_x, positions_y))\n",
    "\n",
    "    # fluxes\n",
    "    F = np.random.uniform(10**(-9), 10**(-4.5), n_sources)\n",
    "    fluxes = F**(-2./3.)\n",
    "    \n",
    "    # Generate the noiseless reference image\n",
    "    ref_noiseless, F_frac = MakeFake(N=1, size=size, n_sources=n_sources,\n",
    "                                     psf_sigma=phi_r, sky=sky,\n",
    "                                     positions=positions, fluxes=fluxes, shifts=[0, 0])\n",
    "    \n",
    "    print('Reference properties')\n",
    "    print('Reference size:', size)\n",
    "    print('Number of sources:', n_sources)\n",
    "    print('PSF standard deviation:', phi_r)\n",
    "    print('Sky level:', sky)\n",
    "    print('F_max/F_total:', F_frac) # flux ratio of brightest star that of all stars\n",
    "    \n",
    "    print('Target kernel properties:')\n",
    "    phi_k = np.random.uniform(0.5, 2.5, 1)[0]\n",
    "    kernel_size = 19\n",
    "    kernel_size = (np.ceil(kernel_size) // 2 * 2 + 1).astype(int) # round up to nearest odd integer\n",
    "    print('Kernel standard deviation:', phi_k)\n",
    "    print('Kernel size:', kernel_size)\n",
    "\n",
    "    # Generate the noiseless (and shifted) target image\n",
    "    phi_i = np.sqrt(phi_k**2 + phi_r**2)\n",
    "    \n",
    "    # positions\n",
    "    shift_x = np.random.uniform(-0.5, 0.5, 1)\n",
    "    shift_y = np.random.uniform(-0.5, 0.5, 1)\n",
    "    #shift_x, shift_y = 0, 0\n",
    "    print('shift_x, shift_y:', shift_x, shift_y)\n",
    "    positions_x_shifted = positions_x + shift_x\n",
    "    positions_y_shifted = positions_y + shift_y\n",
    "    positions_shifted = np.hstack((positions_x_shifted, positions_y_shifted))\n",
    "\n",
    "    imag_noiseless, F_frac = MakeFake(N=1, size=size, n_sources=n_sources,\n",
    "                                     psf_sigma=phi_i, sky=sky,\n",
    "                                     positions=positions_shifted, fluxes=fluxes,\n",
    "                                     shifts = [shift_x[0], shift_y[0]])    \n",
    "    \n",
    "\n",
    "    imag_noiseless_copy = np.copy(imag_noiseless)\n",
    "    ref_noiseless_copy = np.copy(ref_noiseless)\n",
    "    \n",
    "    # read noise [ADU]\n",
    "    sigma_0 = 5.\n",
    "    \n",
    "    # add noise to the reference image\n",
    "    ref, sigma_ref = add_less_noise_to_image(image=ref_noiseless, read_noise=sigma_0)\n",
    "        \n",
    "    # add noise to the target image i.e.\n",
    "    imag, sigma_imag = add_noise_to_image(image=imag_noiseless, read_noise=sigma_0) # need noise map for PSF fitting later on\n",
    "    \n",
    "    print('Reference and target image shapes:')\n",
    "    print(ref.shape, imag.shape)\n",
    "    \n",
    "    # calculate SNR of images\n",
    "    SNR_ref = np.sum(ref_noiseless_copy - sky) / np.sqrt(np.sum(sigma_ref**2))\n",
    "    SNR_imag = np.sum(imag_noiseless_copy - sky) / np.sqrt(np.sum(sigma_imag**2))\n",
    "    print('Reference SNR:', SNR_ref)\n",
    "    print('Target SNR:', SNR_imag)\n",
    "    \n",
    "    # plot the image pair\n",
    "    '''\n",
    "    f, axarr = plt.subplots(1,2)\n",
    "    axarr[0].imshow(ref)\n",
    "    axarr[0].set_title('Reference Image')\n",
    "    axarr[1].imshow(imag)\n",
    "    axarr[1].set_title('Target Image')\n",
    "    plt.show();\n",
    "    '''\n",
    "    \n",
    "    # exit if SNR_imag > 1000\n",
    "    if SNR_imag > 1000:\n",
    "        print('Target SNR regime out of bounds! Skipping...')\n",
    "        continue\n",
    "    \n",
    "    # 'sky' subtract reference... **crucial** for getting past\n",
    "    # the strong anticorrelation between P and B0\n",
    "    print(\"Sky subtracting reference.\")\n",
    "    #ref -= np.median(ref)\n",
    "    ref -= sky\n",
    "\n",
    "    for i in range(0, n_iters):\n",
    "               \n",
    "        if i == 0:\n",
    "            \n",
    "        # for first pass, estimate weights with inverse variance map\n",
    "            weights = 1./(imag + sigma_0**2)\n",
    "        else:\n",
    "            weights = 1./(M + sigma_0**2)\n",
    "            \n",
    "        if i < 3:\n",
    "            \n",
    "            print('\\nB08 solution, iteration %d/%d' % (i+1, 3))\n",
    "            \n",
    "            '''\n",
    "            # extend boundaries of images for B08 solution\n",
    "            ext_ref = extend_image(ref, kernel_size)\n",
    "            ext_imag = extend_image(imag, kernel_size)\n",
    "            ext_weights = extend_image(weights, kernel_size)\n",
    "\n",
    "            U, b = construct_kernel_and_matrices(kernel_size, ext_ref, ext_imag, ext_weights)\n",
    "            kernel, B0 = lstsq_solution(ext_ref, ext_imag, U, b, kernel_size)\n",
    "            '''\n",
    "            \n",
    "            # extend boundaries of images for B08 solution\n",
    "            ext_ref = extend_image_hw(ref, kernel_size)\n",
    "            ext_imag = extend_image_hw(imag, kernel_size)\n",
    "            ext_weights = extend_image_hw(weights, kernel_size)\n",
    "\n",
    "            U, b = construct_kernel_and_matrices(kernel_size, ext_ref, ext_imag, ext_weights)\n",
    "            kernel, B0 = lstsq_solution(ext_ref, ext_imag, U, b, kernel_size)            \n",
    "            \n",
    "            if i == 2:\n",
    "                # compute fit quality and photometric accuracy metrics on this iteration\n",
    "                pyDANDIA = True\n",
    "                #plt.imshow(kernel)\n",
    "                #plt.colorbar();\n",
    "                #plt.title('B08 kernel')\n",
    "                #plt.show();\n",
    "        \n",
    "            \n",
    "        elif i == 3:\n",
    "            print('\\nPyTorchDIA solution')\n",
    "            pyDANDIA = False\n",
    "            PyTorch = True\n",
    "            SD_steps = 25000\n",
    "            \n",
    "            kernel, B0 = PyTorchDIA_CCD.DIA(ref,\n",
    "                                       imag,\n",
    "                                       np.ones(imag.shape), # flatfield\n",
    "                                       rdnoise=sigma_0,\n",
    "                                       G = 1,\n",
    "                                       ks = kernel_size,\n",
    "                                       lr_kernel = 1e-3,\n",
    "                                       lr_B = 10,\n",
    "                                       max_iterations = 25000,\n",
    "                                       poly_degree=0,\n",
    "                                       alpha = 0.,\n",
    "                                       Newton_tol = 1e-6,\n",
    "                                       tol = 1e-9,\n",
    "                                       fast=True,\n",
    "                                       fisher=False,\n",
    "                                       show_convergence_plots=False)\n",
    "            \n",
    "            \n",
    "            #plt.imshow(kernel)\n",
    "            #plt.show()\n",
    "            \n",
    "            # avoid nans\n",
    "            if np.any(np.isnan(kernel)) is True:\n",
    "                print('NaNs in kernel')\n",
    "                continue\n",
    "\n",
    "            #if np.isnan(np.sum(kernel)) == True:\n",
    "            #    continue\n",
    "      \n",
    "     \n",
    "        ## compute model image ##\n",
    "        ## N.B. we extend the border of ref to handle edge-effects associated with the convolution\n",
    "        '''\n",
    "        ext_ref = extend_image(ref, kernel_size)\n",
    "        ext_M = model_image(ext_ref, kernel, B0)\n",
    "        M = ext_M[kernel_size:ext_M.shape[0]-kernel_size, kernel_size:ext_M.shape[1]-kernel_size]\n",
    "        '''\n",
    "        ext_ref = extend_image_hw(ref, kernel_size)\n",
    "        ext_M = model_image(ext_ref, kernel, B0)\n",
    "        hwidth = np.int((kernel_size - 1) / 2)\n",
    "        M = ext_M[hwidth:ext_M.shape[0]-hwidth, hwidth:ext_M.shape[1]-hwidth]        \n",
    "\n",
    "        if np.any(M < 0.) is True:\n",
    "            print('Negatives in Model Image! Fitting failed!')\n",
    "            print('Skipping simulation...')\n",
    "            break\n",
    "        \n",
    "        ## on the final iteration only, compute fit metrics,\n",
    "        ## and perform PSF fitting photometry\n",
    "        ## at the position of the brightest star\n",
    "        if pyDANDIA == True or PyTorch == True:\n",
    "            \n",
    "            # print best fit parameters\n",
    "            print('\\nPhotometric Scale Factor:', np.sum(kernel))\n",
    "            print('B_0:', B0)\n",
    "\n",
    "\n",
    "            # Fit quality metrics\n",
    "            MSE = calc_MSE(M, imag_noiseless, kernel_size)\n",
    "            MFB, MFV = calc_MFB_and_MFV(M, imag, sigma_imag, kernel_size)\n",
    "            #MSEs, MFBs, MFVs = np.append(MSEs, MSE), np.append(MFBs, MFB), np.append(MFVs, MFV)\n",
    "            print('\\nQuality Metrics:')\n",
    "            print('MSE', MSE)\n",
    "            print('MFB', MFB)\n",
    "            print('MFV', MFV)\n",
    "            print('\\n')\n",
    "\n",
    "            # compute the difference image / fit residuals and inferred pixel_variances\n",
    "            D = imag - M\n",
    "            \n",
    "            \n",
    "            # inspect normalised residuals against a unit gaussian\n",
    "            pixel_variances = sigma_0**2 + M # G=1, F_ij=1\n",
    "            \n",
    "            '''\n",
    "            norm_resids = D / np.sqrt(pixel_variances)\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.hist(norm_resids.flatten(), bins='auto', density=True)\n",
    "            x = np.linspace(-5, 5, 100)\n",
    "            plt.plot(x, norm.pdf(x, 0, 1))\n",
    "            plt.xlim(-5, 5)\n",
    "            plt.xticks(fontsize=20)\n",
    "            plt.yticks(fontsize=20)\n",
    "            plt.xlabel('Normalised residuals', fontsize=20)\n",
    "            plt.ylabel('Probability', fontsize=20)\n",
    "            plt.show();\n",
    "            '''\n",
    "            \n",
    "            # evaluate the log-likelihood of the data under the (MLE) model\n",
    "            ll = evaluate_log_likelihood(imag, M, pixel_variances)\n",
    "            print('Log-likelihood:', ll)\n",
    "            \n",
    "            ## PSF fitting photometry of brightest, central star ##\n",
    "            stamp_size = phi_i * 9 # i.e. about 4*target_FWHM\n",
    "            stamp_size = (np.ceil(stamp_size) // 2 * 2 + 1).astype(int) # round up to nearest odd integer\n",
    "            \n",
    "            psf_object = normalised_psf_object(phi_r, stamp_size, [0., 0.])\n",
    "            true_target_psf_object = normalised_psf_object(phi_i, stamp_size, [shift_x[0], shift_y[0]])\n",
    "\n",
    "            # convolve normalised psf_object with the kernel and re-normalise\n",
    "            # remember to deal with the borders appropriately!\n",
    "            ext_psf_object = extend_image(psf_object, kernel_size)\n",
    "            ext_target_psf_object = convolve2d(ext_psf_object, kernel, mode='same')\n",
    "            target_psf_object = ext_target_psf_object[kernel_size:ext_target_psf_object.shape[0]-kernel_size,\n",
    "                                                      kernel_size:ext_target_psf_object.shape[1]-kernel_size]\n",
    "            \n",
    "            target_psf_object = target_psf_object / np.sum(target_psf_object)\n",
    "            \n",
    "            # make stamp around position of bright star in D\n",
    "            stamp = cutout(D, stamp_size)\n",
    "\n",
    "            # cutout target image pixel noise to weight the fit\n",
    "            noise_stamp = cutout(sigma_imag, stamp_size)\n",
    "\n",
    "            # initialise fit parameters: the difference flux and additive constant\n",
    "            F_diff = torch.nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "            const = torch.nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "\n",
    "            # convert numpy.ndarray to torch.Tensor\n",
    "            target_psf_object = torch.from_numpy(target_psf_object)\n",
    "            stamp = torch.from_numpy(stamp)\n",
    "            noise_stamp = torch.from_numpy(noise_stamp)\n",
    "            \n",
    "            # chi-square likelihood, as pixel variances known a priori\n",
    "            class log_likelihood(torch.nn.Module):\n",
    "                def forward(model, stamp, noise_stamp):\n",
    "                    loglikelihood = -0.5*(((stamp - model)/noise_stamp)**2).sum()\n",
    "                    return -loglikelihood\n",
    "            \n",
    "            # initialise optimizer\n",
    "            optimizer = torch.optim.Adam([F_diff, const], lr=10)\n",
    "\n",
    "            tol = 1e-9\n",
    "            losses = []\n",
    "            F_diffs = []\n",
    "            \n",
    "            # optimally scale F_diff to the target_psf_object\n",
    "            for i in range(0, 1000000):\n",
    "                optimizer.zero_grad()\n",
    "                model = F_diff*target_psf_object + const\n",
    "                F_diffs.append(F_diff.item())\n",
    "                loss = log_likelihood.forward(model, stamp, noise_stamp)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if i>1 and abs((losses[-1] - losses[-2])/losses[-2]) < tol:\n",
    "                    print('Converged')\n",
    "                    break\n",
    "                \n",
    "\n",
    "            print('Fitted F_diff and const:', F_diff, const)\n",
    "             \n",
    "            ## convert tensors back to numpy arrays\n",
    "            F_diff = F_diff.detach().numpy()\n",
    "            const = const.detach().numpy()\n",
    "            target_psf_object = target_psf_object.detach().numpy()\n",
    "            stamp = stamp.detach().numpy()\n",
    "            noise_stamp = noise_stamp.detach().numpy()\n",
    "            \n",
    "            ## compute normalised residuals\n",
    "            prediction = F_diff*target_psf_object + const\n",
    "            residuals_stamp = (prediction - stamp) / noise_stamp\n",
    "            \n",
    "            ## inspect images as .fits\n",
    "            #save_numpy_as_fits(D/np.sqrt(pixel_variances), 'residuals_D.fits')\n",
    "            #save_numpy_as_fits(stamp, 'stamp.fits')\n",
    "            #save_numpy_as_fits(cutout(imag, stamp_size), 'imag_stamp.fits')\n",
    "            #save_numpy_as_fits(target_psf_object, 'target_psf_object.fits')\n",
    "            #save_numpy_as_fits(true_target_psf_object, 'true_target_psf_object.fits')\n",
    "            #save_numpy_as_fits(residuals_stamp, 'residuals_stamp.fits')\n",
    "            \n",
    "\n",
    "            ## Compute F_measured of brightest star\n",
    "            F_measured = F_diff / np.sum(kernel)\n",
    "\n",
    "            # Compute the theoretical minimum variance for F_measured\n",
    "            P_true = 1.\n",
    "            var_min = (1./P_true**2) * (np.sum((true_target_psf_object**2)/(noise_stamp**2)))**(-1)\n",
    "            print('F_measured/sigma_min:', F_measured/np.sqrt(var_min))\n",
    "\n",
    "            out = np.vstack((np.sum(kernel), B0, MSE, MFB, MFV, F_measured, var_min,\n",
    "                            star_density, phi_r, sky, phi_k,\n",
    "                            SNR_ref, SNR_imag, F_frac, shift_x, shift_y, ll))\n",
    "            \n",
    "            #path = '/media/james/Seagate_Expansion_Drive#2'\n",
    "            path = os.getcwd()\n",
    "            \n",
    "            if pyDANDIA == True:\n",
    "                filename = os.path.join(path, 'pyDANDIA_December2020_JI.txt')\n",
    "                with open(filename, 'a') as f:\n",
    "                    np.savetxt(f, out.T)\n",
    "            elif PyTorch == True:\n",
    "                filename = os.path.join(path, 'PyTorch_December2020_JI.txt')\n",
    "                with open(filename, 'a') as f:\n",
    "                    np.savetxt(f, out.T)            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Photometric Scale Factor: 0.9415560659567362\n",
    "B_0: 734.6225449273201\n",
    "\n",
    "Quality Metrics:\n",
    "MSE 748.5549165854269\n",
    "MFB -4.9803987601973255e-05\n",
    "MFV 0.9789189191414182\n",
    "\n",
    "\n",
    "Photometric Scale Factor: 0.9415560659567362\n",
    "B_0: 734.6225449273201\n",
    "\n",
    "Quality Metrics:\n",
    "MSE 748.5549165854269\n",
    "MFB -4.9803987601973255e-05\n",
    "MFV 0.9789189191414182\n",
    "\n",
    "\n",
    "Photometric Scale Factor: 0.9415560659567362\n",
    "B_0: 734.6225449273201\n",
    "\n",
    "Quality Metrics:\n",
    "MSE 748.5549165854269\n",
    "MFB -4.9803987601973255e-05\n",
    "MFV 0.9789189191414182\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
