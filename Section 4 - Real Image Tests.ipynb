{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the Real (EMCCD) Image Tests in Section 4 of the manuscript. Why EMCCD images? The majority of collaborators on this project are affiliated with the MiNDSTEp consortium, and specifically, the associated microlensing follow-up observing campaign with the Lucky Imaging camera on the Danish 1.54 m (ESO, La Silla). As such, there's a strong motivation to test how well PyTorchDIA performs on these high frame rate imaging data. While the PSF is always very well sampled, it can take irregular forms, resulting in complicated convolution kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "XAKDgZV8OJru",
    "outputId": "ef6f85ca-3faa-46b2-e0d7-a310e4e36e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.6.0\n",
      "PyTorch version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from astropy.io import fits\n",
    "from astropy.io.fits import getdata\n",
    "from astropy.stats import mad_std\n",
    "from photutils import DAOStarFinder\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import math\n",
    "import PyTorchDIA_Newton # this is an old version of the PyTorchDIA code - used for the 'PSF' inference\n",
    "import PyTorchDIA_EMCCD\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to True for reproducibility\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells setup the Cython routines required for the pyDANDIA implementation of the B08 algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "DTYPE = np.float64\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "\n",
    "# compile suggestion: gcc -shared -pthread -fPIC -fwrapv -O2 -Wall -fno-strict-aliasing -I/somepath/include/python2.7 -o umatrix_routine.so umatrix_routine.c\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking\n",
    "@cython.wraparound(False)  # turn off negative index wrapping\n",
    "@cython.nonecheck(False)  # turn off negative index wrapping\n",
    "\n",
    "def umatrix_construction(np.ndarray[DTYPE_t, ndim = 2] reference_image,np.ndarray[DTYPE_t, ndim = 2] weights, pandq, n_kernel_np, kernel_size_np):\n",
    "\n",
    "    cdef int ni_image = np.shape(reference_image)[0]\n",
    "    cdef int nj_image = np.shape(reference_image)[1]\n",
    "    cdef double sum_acc = 0.\n",
    "    cdef int idx_l,idx_m,idx_l_prime,idx_m_prime,idx_i,idx_j\n",
    "    cdef int kernel_size = np.int(kernel_size_np)\n",
    "    cdef int kernel_size_half = np.int(kernel_size_np)/2\n",
    "    cdef int n_kernel = np.int(n_kernel_np)\n",
    "    cdef np.ndarray u_matrix = np.zeros([n_kernel + 1, n_kernel + 1], dtype=DTYPE)\n",
    "\n",
    "    for idx_p in range(n_kernel):\n",
    "        for idx_q in range(idx_p,n_kernel):\n",
    "            sum_acc = 0.\n",
    "            idx_l, idx_m = pandq[idx_p]\n",
    "            idx_l_prime, idx_m_prime = pandq[idx_q]\n",
    "            for idx_i in range(kernel_size_half,ni_image-kernel_size+kernel_size_half+1):\n",
    "                for idx_j in range(kernel_size_half,nj_image-kernel_size+kernel_size_half+1):\n",
    "                    sum_acc += reference_image[idx_i + idx_l, idx_j + idx_m] * reference_image[idx_i + idx_l_prime,idx_j + idx_m_prime]  * weights[idx_i, idx_j]\n",
    "            u_matrix[idx_p, idx_q] = sum_acc\n",
    "            u_matrix[idx_q, idx_p] = sum_acc\n",
    "\n",
    "    for idx_p in [n_kernel]:\n",
    "        for idx_q in range(n_kernel):\n",
    "            sum_acc = 0.\n",
    "            idx_l = kernel_size\n",
    "            idx_m = kernel_size\n",
    "            idx_l_prime, idx_m_prime = pandq[idx_q]\n",
    "            for idx_i in range(kernel_size_half,ni_image-kernel_size+kernel_size_half+1):\n",
    "                for idx_j in range(kernel_size_half,nj_image-kernel_size+kernel_size_half+1):\n",
    "                    sum_acc += reference_image[idx_i + idx_l_prime, idx_j + idx_m_prime] * weights[idx_i, idx_j]\n",
    "            u_matrix[idx_p, idx_q] = sum_acc\n",
    "    \n",
    "    for idx_p in range(n_kernel):\n",
    "        for idx_q in [n_kernel]:\n",
    "            sum_acc = 0.\n",
    "            idx_l, idx_m = pandq[idx_p]\n",
    "            idx_l_prime = kernel_size\n",
    "            idl_m_prime = kernel_size\n",
    "            for idx_i in range(kernel_size_half,ni_image-kernel_size+kernel_size_half+1):\n",
    "                for idx_j in range(kernel_size_half, nj_image-kernel_size+kernel_size_half+1):\n",
    "                    sum_acc += reference_image[idx_i + idx_l, idx_j + idx_m] * weights[idx_i, idx_j] \n",
    "            u_matrix[idx_p, idx_q] = sum_acc\n",
    "\n",
    "    sum_acc = 0.\n",
    "    for idx_i in range(ni_image):\n",
    "        for idx_j in range(nj_image):\n",
    "            sum_acc += weights[idx_i, idx_j] \n",
    "    u_matrix[n_kernel, n_kernel] = sum_acc\n",
    "    \n",
    "    return u_matrix\n",
    "\n",
    "def bvector_construction(np.ndarray[DTYPE_t, ndim = 2] reference_image,np.ndarray[DTYPE_t, ndim = 2] data_image,np.ndarray[DTYPE_t, ndim = 2] weights, pandq, n_kernel_np, kernel_size_np):\n",
    "\n",
    "    cdef int ni_image = np.shape(data_image)[0]\n",
    "    cdef int nj_image = np.shape(data_image)[1]\n",
    "    cdef double sum_acc = 0.\n",
    "    cdef int idx_l,idx_m,idx_l_prime,idx_m_prime,idx_i,idx_j\n",
    "    cdef int kernel_size = np.int(kernel_size_np)\n",
    "    cdef int kernel_size_half = np.int(kernel_size_np)/2\n",
    "    cdef int n_kernel = np.int(n_kernel_np)\n",
    "        \n",
    "    cdef np.ndarray b_vector = np.zeros([n_kernel + 1], dtype=DTYPE)\n",
    "    for idx_p in range(n_kernel):\n",
    "        idx_l, idx_m = pandq[idx_p]\n",
    "        sum_acc = 0.\n",
    "        for idx_i in range(kernel_size_half,ni_image-kernel_size+kernel_size_half+1):\n",
    "            for idx_j in range(kernel_size_half,nj_image-kernel_size+kernel_size_half+1):\n",
    "                   sum_acc += data_image[idx_i, idx_j] * reference_image[idx_i + idx_l , idx_j + idx_m ] * weights[idx_i, idx_j]\n",
    "        b_vector[idx_p] = sum_acc\n",
    "\n",
    "    sum_acc = 0.\n",
    "    for idx_i in range(ni_image):\n",
    "        for idx_j in range(nj_image):\n",
    "            sum_acc += data_image[idx_i, idx_j] * weights[idx_i, idx_j]\n",
    "    b_vector[n_kernel] = sum_acc\n",
    "\n",
    "    return b_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build the kernel, U matrix and b vector\n",
    "def construct_kernel_and_matrices(kernel_size, R, I, weights):\n",
    "\n",
    "    pandq = []\n",
    "    n_kernel = kernel_size * kernel_size\n",
    "    ncount = 0\n",
    "    half_kernel_size = int(int(kernel_size) / 2)\n",
    "    for lidx in range(kernel_size):\n",
    "        for midx in range(kernel_size):\n",
    "            pandq.append((lidx - half_kernel_size, midx - half_kernel_size))\n",
    "\n",
    "\n",
    "    R = R.astype('float64')\n",
    "    I =  I.astype('float64')\n",
    "    weights = weights.astype('float64')\n",
    "\n",
    "    start_time = time.time()\n",
    "    U = umatrix_construction(R, weights, pandq, n_kernel, kernel_size)\n",
    "    b = bvector_construction(R, I, weights, pandq, n_kernel, kernel_size)\n",
    "    print(\"--- Finished U and b construction in %s seconds ---\" % (time.time() - start_time))\n",
    "    return U, b\n",
    "\n",
    "\n",
    "# define a function for the least-squares solution\n",
    "def lstsq_solution(R, I, U, b, kernel_size):\n",
    "    \n",
    "    lstsq_result = np.linalg.lstsq(np.array(U), np.array(b), rcond=None)\n",
    "    a_vector = lstsq_result[0]\n",
    "    lstsq_fit = np.dot(np.array(U), a_vector)\n",
    "    resid = np.array(b) - lstsq_fit\n",
    "    reduced_chisqr = np.sum(resid ** 2) / (float(kernel_size * kernel_size))\n",
    "    lstsq_cov = np.dot(np.array(U).T, np.array(U)) * reduced_chisqr\n",
    "    resivar = np.var(resid, ddof=0) * float(len(a_vector))\n",
    "    \n",
    "    # use pinv in order to stabilize calculation\n",
    "    a_var = np.diag(np.linalg.pinv(lstsq_cov) * resivar)\n",
    "\n",
    "    a_vector_err = np.sqrt(a_var)\n",
    "    output_kernel = np.zeros(kernel_size * kernel_size, dtype=float)\n",
    "    if len(a_vector) > kernel_size * kernel_size:\n",
    "        output_kernel = a_vector[:-1]\n",
    "    else:\n",
    "        output_kernel = a_vector\n",
    "    output_kernel = output_kernel.reshape((kernel_size, kernel_size))\n",
    "\n",
    "    err_kernel = np.zeros(kernel_size * kernel_size, dtype=float)\n",
    "    if len(a_vector) > kernel_size * kernel_size:\n",
    "        err_kernel = a_vector_err[:-1]\n",
    "        err_kernel = err_kernel.reshape((kernel_size, kernel_size))\n",
    "    else:\n",
    "        err_kernel = a_vector_err\n",
    "        err_kernel = err_kernel.reshape((kernel_size, kernel_size))\n",
    "\n",
    "    output_kernel_2 = np.flip(np.flip(output_kernel, 0), 1)\n",
    "    err_kernel_2 = np.flip(np.flip(err_kernel, 0), 1)\n",
    "    bkg_kernel = a_vector[-1]\n",
    "    output_kernel_2.shape\n",
    "\n",
    "    return output_kernel_2, bkg_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's move on to our data set. This consists of 250 EMCCD  images, each of which is formed of as many as 3000 shift-and-stacked 0.1 second exposures (i.e. approx. 5 min total integration time). Each image is 512x512 pixels, with scale of 0.09 arcsecond/pixel. First, I'll grab the reference image - this was identified as the 'best' by the associated routine in the pyDANDIA pipeline, and is also the sharpest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path to images\n",
    "path = 'OGLE-III-BLG101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DJ1PDV3UOVu4",
    "outputId": "6163f8fd-b4a2-473c-faf9-20263d152c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference FWHM: 5.625057500773691\n",
      "Reference TOT_IM: 3000.0\n",
      "Reference shape: 512 512\n"
     ]
    }
   ],
   "source": [
    "## reference image\n",
    "ref_file = os.path.join(path, 'coll_OGLE-III-BLG101_Llr_2019-07-20_00048.fits')\n",
    "ref_data = getdata(ref_file, header=True)\n",
    "ref, ref_fwhm, ref_totim = ref_data[0], ref_data[1]['FWHM'], ref_data[1]['TOT_IM']\n",
    "nx, ny = ref.shape\n",
    "print('Reference FWHM:', ref_fwhm)\n",
    "print('Reference TOT_IM:', ref_totim) #  total number of shift-and-stacked exposures (max=3000)\n",
    "print('Reference shape:', nx, ny) # pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these tests we will need to perform PSF-fitting photometry to assess the photometric accuracy of the B08 and PyTorchDIA approaches. We therefore need an accurate model PSF in these images. Due to the complicated nature of the Lucky Imager's PSF, analytical models such as Gaussians and Moffat's won't cut it; we need to construct an empirical model!\n",
    "\n",
    "It turns out that we can use some flavour of PyTorchDIA to do just this. If we were to replace our reference image with (approximate) delta-functions only at the positions of the stars and set all other pixels to 0, the associated 'kernel' that convolves this representation of the scene to any given data image should be a reasonable model for the PSF of that data image. Let's start by constructing this 'delta-function' scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find bright star peaks above given threshold (sky subtract)\n",
    "## we'll also use this source list in the next cell to identify\n",
    "## suitable stars for the PSF fitting photometry.\n",
    "sky, std, thrs = np.median(ref), mad_std(ref), 5\n",
    "daofind_bright = DAOStarFinder(fwhm=ref_fwhm, threshold=thrs*std)  \n",
    "sources = daofind_bright(ref - sky)\n",
    "\n",
    "ref_delta_positions = np.transpose((sources['xcentroid'], sources['ycentroid'], sources['peak']))\n",
    "\n",
    "# delta function positions\n",
    "dbf_x = ref_delta_positions[:,0].astype(int)\n",
    "dbf_y = ref_delta_positions[:,1].astype(int)\n",
    "\n",
    "# mask non peak positions\n",
    "mask_image = np.zeros(ref.shape)\n",
    "mask_image[dbf_y, dbf_x] = 1\n",
    "\n",
    "# apply mask to a copy of the reference image\n",
    "ref_delta = np.copy(ref)\n",
    "ref_delta[mask_image == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected a total of 232 sources\n",
      "Using 155 sources to assess photometric accuracy.\n"
     ]
    }
   ],
   "source": [
    "print('Detected a total of %d sources' % len(sources))\n",
    "\n",
    "# avoid bad regions at the edges\n",
    "source_table = sources[(sources['xcentroid'] > 50) & (sources['xcentroid'] < (nx - 50))]\n",
    "source_table = source_table[(source_table['ycentroid'] > 50) & (source_table['ycentroid'] < (ny - 50))]\n",
    "\n",
    "print('Using %d sources to assess photometric accuracy.' % len(source_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 250 useful data images.\n"
     ]
    }
   ],
   "source": [
    "## master flat\n",
    "#path = 'LOB190560Z'\n",
    "\n",
    "#flat_file = os.path.join(path, 'master_flat.fits')\n",
    "#master_flat = getdata(flat_file, 0, header=True)[0]\n",
    "\n",
    "## shifts\n",
    "shift_info = os.path.join(path, 'Shift_info.txt')\n",
    "shifts = np.genfromtxt(shift_info, delimiter=\"\\t\", dtype=str) # filename | xs | ys\n",
    "\n",
    "# store some of the house-keeping data\n",
    "fnames = []\n",
    "images = []\n",
    "FWHMs = []\n",
    "Tot_ims = []\n",
    "\n",
    "# avoid reference and a file with a badly incorrect pointing\n",
    "avoid = [ref_file, '2019-07-28_00000.fits']\n",
    "\n",
    "for image_file in glob.glob(os.path.join(path, \"*coll*\")):\n",
    "    if image_file not in avoid:\n",
    "        image_data = getdata(image_file, header=True)\n",
    "        image, header = image_data[0], image_data[1]\n",
    "        # there are some 30 Hz exposures mixed in with the 10 Hz we're interested in,\n",
    "        # so avoid any image where TOT_IM > 3000\n",
    "        if header['TOT_IM'] <= 3000.0:\n",
    "            fnames.append(image_file.split('/')[-1])\n",
    "            images.append(image)\n",
    "            FWHMs.append(header['FWHM'])\n",
    "            Tot_ims.append(header['TOT_IM'])\n",
    "        \n",
    "\n",
    "# we'll also need the master flat fields to include in our noise model\n",
    "master_flats = []\n",
    "master_flat_dates = []\n",
    "for flat_file in glob.glob(os.path.join(path, \"Master_flats_OGLE-III-BLG101/*.fits\")):\n",
    "    flat = getdata(flat_file, header=False)\n",
    "    master_flats.append(flat)\n",
    "    master_flat_dates.append(flat_file.split('_')[-1].split('.')[0])\n",
    "\n",
    "\n",
    "# convert to numpy arrays (float32)\n",
    "#images, FWHMs, Tot_ims = np.array(images, dtype=np.float32), np.array(FWHMs), np.array(Tot_ims, dtype=np.int)\n",
    "#master_flats = np.array(master_flats, dtype=np.float32)\n",
    "print('Found a total of %d useful data images.' % len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some house-keeping functions for padding the images (in order to guard against edge-effects associated with convolution), computing fit quality metrics and for cutting out stamps for the PSF fitting photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend image prior to convoliving with kernel\n",
    "def extend_image(image, kernel_size):\n",
    "    image_extended = np.zeros((np.shape(image)[0] + 2 * kernel_size,\n",
    "                             np.shape(image)[1] + 2 * kernel_size))\n",
    "    image_extended[kernel_size:-kernel_size, kernel_size:-kernel_size] = np.array(image, float)\n",
    "    \n",
    "    return image_extended\n",
    "\n",
    "\n",
    "def extend_image_hw(image, kernel_size):\n",
    "    image_extended = np.zeros((np.shape(image)[0] + kernel_size - 1,\n",
    "                             np.shape(image)[1] + kernel_size - 1))\n",
    "    hwidth = np.int((kernel_size - 1) / 2)\n",
    "    image_extended[hwidth:image_extended.shape[0]-hwidth,\n",
    "                   hwidth:image_extended.shape[1]-hwidth] = np.array(image, float)\n",
    "    return image_extended\n",
    "\n",
    "# define function to return fit quality metrics\n",
    "def metrics(M, I, noise_map, kernel_size, mask):\n",
    "    N_data = len(I[mask == 0].flatten())\n",
    "    MFB = 1./(N_data) * np.sum((I - M)/noise_map)\n",
    "    MFV = 1./(N_data - 1) * np.sum((((I - M)/noise_map) - MFB)**2)\n",
    "    return MFB, MFV\n",
    "\n",
    "# cutout stamp around position of selected stars\n",
    "def make_stamp(image, pos, stamp_size):\n",
    "    rad = np.int(stamp_size/2)\n",
    "    #x_centroid, y_centroid = pos[1], pos[0]\n",
    "    x_centroid, y_centroid = pos[1].astype(int), pos[0].astype(int)\n",
    "    x_max, x_min = x_centroid + rad, x_centroid - rad\n",
    "    y_max, y_min = y_centroid + rad, y_centroid - rad\n",
    "    stamp = image[x_min:x_max+1, y_min:y_max+1]\n",
    "    return stamp\n",
    "\n",
    "# cutout stamp around position of selected stars\n",
    "def make_fit_cutout(image_stamp, c_size):\n",
    "    centre = np.int(image_stamp.shape[0]/2)\n",
    "    radius = np.int((c_size/2))\n",
    "    cutout = image_stamp[centre - radius:centre + radius + 1, centre - radius:centre + radius + 1]\n",
    "    return cutout\n",
    "\n",
    "def evaluate_gaussian_log_likelihood(data, model, var):\n",
    "    #L(data|model_x) = - ( 1/2 * \\chi^2 + \\sum_{i=1}^{N} \\ln \\sigma_i + N/4 * \\ln (2*\\pi) ) ,)\n",
    "    chi2 = (data - model)**2 / var\n",
    "    lnsigma = 2 * np.log(var)\n",
    "    norm_constant = (len(data.flatten()) / 4) * np.log(2 * np.pi)\n",
    "    \n",
    "    return -np.sum(chi2 + lnsigma + norm_constant)\n",
    "\n",
    "def evaluate_huber_log_likelihood(data, model, var, c=1.345):\n",
    "    ## PyTorchDIA - 'Huber' likelihood\n",
    "\n",
    "    NM = np.sqrt(var)\n",
    "    ln_sigma = np.log(NM).sum()\n",
    "\n",
    "    # gaussian when (model - targ)/NM <= c\n",
    "    # absolute deviation when (model - targ)/NM > c\n",
    "    cond1 = np.abs((model - data)/NM) <= c\n",
    "    cond2 = np.abs((model - data)/NM) > c\n",
    "    inliers = ((model - data)/NM)[cond1]\n",
    "    outliers = ((model - data)/NM)[cond2]\n",
    "\n",
    "    l2 = 0.5*(np.power(inliers, 2)).sum()\n",
    "    l1 = (c *(np.abs(outliers)) - (0.5 * c**2)).sum()\n",
    "    \n",
    "    # N.B. If we recast our Huber loss function as a valid likelihood function,\n",
    "    # a sigma term enters the normalisation constant (Q in Eq. 11 of manuscript), so really,\n",
    "    # we should be minimising l2 + l1 + 2*ln_sigma. We've however constructed this loss\n",
    "    # function to be identical to a Gaussian for inlying data, hence the\n",
    "    # l2 + l1 + ln_sigma used in the PyTorchDIA_EMCCD script.\n",
    "    # sqrt(2)*sqrt(pi)*erf(sqrt(2)*c/2)/sigma + 2*exp(-c**2/2)/(c*sigma)\n",
    "    const = (np.sqrt(2*np.pi)*c*math.erf(c/np.sqrt(2)) + 2*np.exp(-c**2/2)) / c\n",
    "    lnQ = len(data.flatten())*np.log(const) - np.log(NM).sum() # i.e. this is where the 2*ln_sigma creeps in...\n",
    "    \n",
    "    ll = -(l2 + l1 + ln_sigma) + lnQ\n",
    "    # -np.sum(l2 + l1 + ln_sigma) + np.sum(lnQ)\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to perform difference imaging on this data set! We do not resample the data images to the reference image, and so avoid introducing correlated pixel noise into our data. Instead, we cutout 100x100 pixel subregions centered on the stars identified above, to within the nearest integer pixel shift. One crucial advantage of the discrete pixel array kernel that both the B08 and PyTorchDIA models use, is that subpixel shifts between images can be corrected for in the kernel solution. Consequently, if we convolve a PSF centered at the position of some star in the reference image with the inferred kernel, the result is a PSF for the data image at the position of the star in this data image. No resampling to perfectly register the star positions is required - the kernel takes care of that for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sky level of reference [ADU]: 6439.2393\n"
     ]
    }
   ],
   "source": [
    "## 'sky' subtract reference - over this 45x45 arcsecond FOV, a simple scalar subtraction is fine\n",
    "## even for this fairly crowded field. There is a gradient in the backgrond at the LI camera edges,\n",
    "## but we've avoided those regions as part of the star selection criteria above.\n",
    "sky = np.median(ref)\n",
    "ref -= sky\n",
    "print('Sky level of reference [ADU]:', sky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab positions of the 'good' stars\n",
    "positions = np.transpose((source_table['xcentroid'], source_table['ycentroid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4UGBx4nDRT20",
    "outputId": "e128c581-9498-4c14-e37f-145a2bad8eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0 / 250\n",
      "coll_OGLE-III-BLG101_Llr_2019-07-26_00010.fits\n",
      "Using flat: 2019-07-26\n",
      "Image FWHM: 10.05790566350711\n",
      "Stamp and kernel size: 21\n",
      "Source position in reference frame: [219.94513057  52.63611808]\n",
      "Integer shift (x, y): [13 -2]\n",
      "\n",
      "Masked pixels (pyDANDIA): 0.0\n",
      "\n",
      "pyDANDIA solution, iter 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c6a4e8071ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mext_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_image_hw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_stamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_kernel_and_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_imag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0mkernel_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB0_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstsq_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_imag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'P:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_pd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e01a77262b33>\u001b[0m in \u001b[0;36mconstruct_kernel_and_matrices\u001b[0;34m(kernel_size, R, I, weights)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumatrix_construction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbvector_construction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Finished U and b construction in %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## iterate through target images\n",
    "start_time = time.time()\n",
    "\n",
    "# size of cutouts to make i.e. 100 x 100 pixels\n",
    "cutout_size = 100\n",
    "\n",
    "# EMCCD detector parameters to plug into the noise model\n",
    "# for these long, ~5 minute exposures, shot noise >> readout noise\n",
    "# on this camera, so we can ignore the latter\n",
    "gain_CCD = 25.8 # CCD gain [e-_EM / ADU]\n",
    "gain_EM = 300. # EM gain [e-_EM / e-_phot]\n",
    "G = gain_CCD / gain_EM # Total gain [e-_phot / ADU]\n",
    "excess_noise_factor = 2 # EMCCD fudge factor\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    \n",
    "    ##### pick up from where we last left off ####\n",
    "    if i < 1e99:\n",
    "    \n",
    "        #out_pd = np.vstack((MFBs_pd, MFVs_pd, Ps_normalised_pd, np.array(norm_phot_resids_pd).flatten(),\n",
    "        #                 image_SNRs_pd, image_FWHMs_pd, stamp_ids_pd)).T\n",
    "\n",
    "        image_FWHMs_pd = []\n",
    "        image_SNRs_pd = []\n",
    "        MFBs_pd = []\n",
    "        MFVs_pd = []\n",
    "        Ps_normalised_pd = []\n",
    "        B0s_pd = []\n",
    "        norm_phot_resids_pd = []\n",
    "        stamp_ids_pd = []\n",
    "        ll_pds = []\n",
    "\n",
    "        image_FWHMs_pt = []\n",
    "        image_SNRs_pt = []    \n",
    "        MFBs_pt = []\n",
    "        MFVs_pt = []\n",
    "        Ps_normalised_pt = []\n",
    "        B0s_pt = []\n",
    "        norm_phot_resids_pt = []\n",
    "        stamp_ids_pt = []\n",
    "        ll_pts = []\n",
    "\n",
    "        print('Image %d / %d' % (i, len(images)))\n",
    "        print(fnames[i])\n",
    "\n",
    "        # load corresponding flat\n",
    "        date = fnames[i].split('_')[3]\n",
    "        flat_index = np.where(np.array(master_flat_dates) == date)[0][0]\n",
    "        print('Using flat:', master_flat_dates[flat_index])\n",
    "        master_flat = master_flats[flat_index]\n",
    "\n",
    "        # compute appropriate stamp and kernel size\n",
    "        # the former will be used for PSF fitting photometry\n",
    "        # a square (2 * FWHM) x (2 * FWHM) stamp is a reasonable choice\n",
    "        fwhm = FWHMs[i]\n",
    "        print('Image FWHM:', fwhm)\n",
    "        ks = np.int(2*fwhm)\n",
    "        if ks % 2 == 0: # must be odd!\n",
    "            ks += 1\n",
    "        print('Stamp and kernel size:', ks)\n",
    "        \n",
    "        # this is used for padding the image borders\n",
    "        hwidth = np.int((ks - 1) / 2)\n",
    "\n",
    "        # find integer shifts\n",
    "        ys = shifts[:,1][np.where(shifts[:,0] == fnames[i])][0] \n",
    "        xs = shifts[:,2][np.where(shifts[:,0] == fnames[i])][0]\n",
    "\n",
    "        # x and y need to be switched\n",
    "        shiftxy = np.array([xs.astype(int), ys.astype(int)])\n",
    "\n",
    "        # store values for photometric accuracy metrics\n",
    "        # requires relative P for P_true\n",
    "        photometric_scale_factors_pd = []\n",
    "        F_measured_values_pd = []\n",
    "        photometric_scale_factors_pt = []\n",
    "        F_measured_values_pt = []    \n",
    "        target_psf_objects = []\n",
    "        pixel_uncertanties_list = []\n",
    "\n",
    "\n",
    "        ## for each image, divide this into image and reference stamps centered on a star ##\n",
    "        ## solve a kernel and background term for each, and perform psf fitting at the    ##\n",
    "        ## position of the star in the difference image.\n",
    "\n",
    "        ## deal with nans on first pass ##\n",
    "        for j,pos in enumerate(positions[:, [0,1]]):\n",
    "\n",
    "            try:\n",
    "\n",
    "                # add shifts to positions so we get the right part of the reference and flat\n",
    "                print('Source position in reference frame:', pos)\n",
    "                print('Integer shift (x, y):', shiftxy)\n",
    "                ref_stamp = make_stamp(ref, pos, cutout_size)\n",
    "                image_stamp = make_stamp(image, pos + shiftxy, cutout_size)\n",
    "                flat_stamp = make_stamp(master_flat, pos + shiftxy, cutout_size)\n",
    "                ref_delta_stamp = make_stamp(ref_delta, pos, cutout_size)\n",
    "                \n",
    "                ################################################################################\n",
    "                ######################## pyDANDIA ##############################################\n",
    "                ## run pyDANDIA to build the bad pixel mask and pixel uncertanties so we compare\n",
    "                ## like with like when calculating metrics\n",
    "                mask = np.zeros(image_stamp.shape) # bad pixel mask\n",
    "                for iters in range(0, 4):\n",
    "\n",
    "                    if iters == 0:\n",
    "                        shot_noise = image_stamp/(G*flat_stamp)\n",
    "                    else:\n",
    "                        shot_noise = model_pd/(G*flat_stamp)   \n",
    "\n",
    "                    pixel_variances = excess_noise_factor*shot_noise\n",
    "                    weights_stamp = 1./pixel_variances\n",
    "                    \n",
    "                    # mask 5-sigma outliers\n",
    "                    if iters > 1:\n",
    "                        # update mask on third and fourth iterations only\n",
    "                        norm_resids = np.sqrt(weights_stamp)*(image_stamp - model_pd)\n",
    "                        mask[np.where(np.abs(norm_resids)>5)] = 1\n",
    "                        \n",
    "                    # weight-out bad pixels\n",
    "                    weights_stamp[np.where(mask == 1)] = 1e-99\n",
    "                    print('\\nMasked pixels (pyDANDIA):', np.sum(mask))\n",
    "                    \n",
    "                    print('\\npyDANDIA solution, iter %d' % iters)\n",
    "                    \n",
    "                    # pad input images by half the kernel size\n",
    "                    ext_ref = extend_image_hw(ref_stamp, ks)\n",
    "                    ext_imag = extend_image_hw(image_stamp, ks)\n",
    "                    ext_weights = extend_image_hw(weights_stamp, ks)\n",
    "                    \n",
    "                    U, b = construct_kernel_and_matrices(ks, ext_ref, ext_imag, ext_weights)\n",
    "                    kernel_pd, B0_pd = lstsq_solution(ext_ref, ext_imag, U, b, ks)\n",
    "                    print('P:', np.sum(kernel_pd))\n",
    "                    print('B0:', B0_pd)\n",
    "\n",
    "                    # compute the model - B08/pyDANDIA\n",
    "                    ext_ref_stamp = extend_image_hw(ref_stamp, ks)\n",
    "                    ext_model_pd = convolve2d(ext_ref_stamp, kernel_pd, mode='same') + B0_pd\n",
    "                    model_pd = ext_model_pd[hwidth:ext_model_pd.shape[0]-hwidth,\n",
    "                                            hwidth:ext_model_pd.shape[1]-hwidth]    \n",
    "\n",
    "\n",
    "                    if iters == 3:\n",
    "                        # calculate pixel uncertanties on the final, fourth iteration\n",
    "                        shot_noise = model_pd/(G*flat_stamp)\n",
    "                        pixel_variances = excess_noise_factor*shot_noise\n",
    "                        pixel_uncertanties = np.sqrt(pixel_variances)\n",
    "\n",
    "\n",
    "                #########################################################################\n",
    "                #########################################################################\n",
    "                \n",
    "                ##################### PyTorchDIA ########################################\n",
    "                # infer kernel via robust PyTorchDIA code\n",
    "                print('\\n(Robust) PyTorchDIA solution')\n",
    "                SD_steps = 25000\n",
    "                kernel_pt, B0_pt = PyTorchDIA_EMCCD.DIA(ref_stamp,\n",
    "                                           image_stamp,\n",
    "                                           flat_stamp,\n",
    "                                           read_noise = 0.,\n",
    "                                           ks = ks,\n",
    "                                           lr_kernel = 1e-3,\n",
    "                                           lr_B = 100,\n",
    "                                           SD_steps = SD_steps,\n",
    "                                           Newton_tol = 1e-6,\n",
    "                                           poly_degree=0,\n",
    "                                           fast=True,\n",
    "                                           tol = 1e-9,\n",
    "                                           max_iterations = SD_steps,\n",
    "                                           fisher=False,\n",
    "                                           show_convergence_plots=False)\n",
    "                \n",
    "                # compute the model - PyTorch\n",
    "                ext_ref_stamp = extend_image_hw(ref_stamp, ks)\n",
    "                ext_model_pt = convolve2d(ext_ref_stamp, kernel_pt, mode='same') + B0_pt\n",
    "                model_pt = ext_model_pt[hwidth:ext_model_pt.shape[0]-hwidth,\n",
    "                                        hwidth:ext_model_pt.shape[1]-hwidth] \n",
    "                \n",
    "                # and variances for evaluating the Huber 'likelihood'\n",
    "                shot_noise_pt = model_pt/(G*flat_stamp)\n",
    "                pixel_variances_pt = excess_noise_factor*shot_noise_pt\n",
    "                ############################################################################\n",
    "                ############################################################################\n",
    "                \n",
    "                ### compute likelihood ratio only if there is no data rejection\n",
    "                if np.sum(mask) == 0:\n",
    "                    ll_pd = evaluate_gaussian_log_likelihood(image_stamp, model_pd, pixel_variances)\n",
    "                    ll_pt = evaluate_huber_log_likelihood(image_stamp, model_pt, pixel_variances_pt, c=1.345)                \n",
    "                    print('pyDANDIA (Gaussian) log-likelihood:', ll_pd)\n",
    "                    print('PyTorch (Huber) log-likelihood:', ll_pt)\n",
    "                    print('ll_pd - ll_pt:', ll_pd - ll_pt)\n",
    "                    \n",
    "                    ll_pds.append(ll_pd)\n",
    "                    ll_pts.append(ll_pt)\n",
    "                    \n",
    "                else:\n",
    "                    ll_pds.append(0)\n",
    "                    ll_pts.append(0)                    \n",
    "\n",
    "                # image models\n",
    "                models = [model_pd, model_pt]\n",
    "\n",
    "\n",
    "                #### PSF fitting photometry in difference image ####\n",
    "                #### infer PSF - this is unweighted ###\n",
    "                res = PyTorchDIA_Newton.DIA(ref_delta_stamp, image_stamp, flat_stamp,\n",
    "                                              tot_im = Tot_ims[i], unweighted=True,\n",
    "                                              iters=1, ks = ks, SD_steps = 0,\n",
    "                                              tol = 1e-9, k=1e99)\n",
    "\n",
    "                psf = res[0]\n",
    "\n",
    "                #stop = input()\n",
    "                psf /= np.sum(psf)\n",
    "\n",
    "                for model in models:\n",
    "\n",
    "                    if model is model_pd:\n",
    "                        print('\\nCalculating pyDANDIA metrics')\n",
    "                    else:\n",
    "                        print('\\nCalculating PyTorchDIA metrics')\n",
    "\n",
    "                    stamp_fit = make_fit_cutout(image_stamp - model, ks)\n",
    "                    noise_stamp_fit = make_fit_cutout(pixel_uncertanties, ks)\n",
    "\n",
    "                    ## psf fit\n",
    "                    F_diff = torch.nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "                    const = torch.nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "                    print('Starting F_diff and const:', F_diff, const)\n",
    "\n",
    "                    #print(model.shape, stamp_fit.shape, noise_stamp_fit.shape)\n",
    "\n",
    "                    target_psf_object = torch.from_numpy(psf)\n",
    "                    stamp_fit = torch.from_numpy(np.array(stamp_fit, dtype=np.float32))\n",
    "                    noise_stamp_fit = torch.from_numpy(np.array(noise_stamp_fit, dtype=np.float32))\n",
    "\n",
    "                    class log_likelihood(torch.nn.Module):\n",
    "\n",
    "                        def forward(model, stamp, noise_stamp):\n",
    "                            #print(stamp.size(), model.size(), noise_stamp.size())\n",
    "                            loglikelihood = -0.5*(((stamp - model)/noise_stamp)**2).sum()\n",
    "                            return -loglikelihood\n",
    "\n",
    "                    optimizer = torch.optim.Adam([F_diff, const], lr=1000)\n",
    "\n",
    "                    tol = 1e-10\n",
    "                    losses = []\n",
    "                    for step in range(0, 150000):\n",
    "                        optimizer.zero_grad()\n",
    "                        psf_model = F_diff*target_psf_object + const\n",
    "                        loss = log_likelihood.forward(psf_model, stamp_fit, noise_stamp_fit)\n",
    "                        losses.append(loss.item())\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if step>1 and abs((losses[-1] - losses[-2])/losses[-2]) < tol:\n",
    "                            print('Converged')\n",
    "                            break\n",
    "\n",
    "\n",
    "                    print('Final F and const:', F_diff, const)\n",
    "                    #plt.plot(losses)\n",
    "\n",
    "                    ## convert tensors back to numpy arrays\n",
    "                    F_diff = F_diff.detach().numpy()\n",
    "                    const = const.detach().numpy()\n",
    "                    target_psf_object = target_psf_object.detach().numpy()\n",
    "                    stamp_fit = stamp_fit.detach().numpy()\n",
    "                    noise_stamp_fit = noise_stamp_fit.detach().numpy()\n",
    "\n",
    "                    '''\n",
    "                    # plot the stamp\n",
    "                    plt.figure()\n",
    "                    plt.title('Difference image stamp')\n",
    "                    plt.imshow(stamp_fit)\n",
    "                    plt.colorbar()\n",
    "                    plt.show()\n",
    "\n",
    "                    # plot the reference stamp - to check aligned OK\n",
    "                    plt.figure()\n",
    "                    plt.title('Reference Image stamp')\n",
    "                    plt.imshow(make_fit_cutout(ref_stamp, ks))\n",
    "                    plt.colorbar()\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "                    # plot the image stamp - to check psf accurate!\n",
    "                    plt.figure()\n",
    "                    plt.title('Data Image stamp')\n",
    "                    plt.imshow(make_fit_cutout(image_stamp, ks))\n",
    "                    plt.colorbar()\n",
    "                    plt.show()\n",
    "\n",
    "                    # plot the psf model\n",
    "                    plt.figure()\n",
    "                    plt.title('Normalised PSF model')\n",
    "                    plt.imshow(target_psf_object)\n",
    "                    plt.colorbar()\n",
    "                    plt.show()\n",
    "\n",
    "                    # plot the residuals\n",
    "                    prediction = F_diff*target_psf_object + const\n",
    "                    residuals = prediction - stamp_fit\n",
    "                    plt.figure()\n",
    "                    plt.title('Residuals from PSF fit to difference image stamp')\n",
    "                    plt.imshow(residuals)\n",
    "                    plt.colorbar()\n",
    "                    plt.show()\n",
    "                    '''\n",
    "\n",
    "                    if model is model_pd:\n",
    "\n",
    "                        ## Measured flux of brightest star\n",
    "                        F_measured_pd = F_diff / np.sum(kernel_pd)\n",
    "\n",
    "                        # store values to cal\n",
    "                        F_measured_values_pd.append(F_measured_pd)\n",
    "                        photometric_scale_factors_pd.append(np.sum(kernel_pd))\n",
    "                        target_psf_objects.append(target_psf_object)\n",
    "                        pixel_uncertanties_list.append(noise_stamp_fit)\n",
    "\n",
    "                        # stamp number\n",
    "                        stamp_ids_pd.append(j)\n",
    "\n",
    "                        # image SNR\n",
    "                        sky = np.median(image_stamp)\n",
    "                        SNR = np.sum(image_stamp - sky) / np.sqrt(np.sum(pixel_uncertanties**2))\n",
    "                        image_SNRs_pd.append(SNR)\n",
    "                        image_FWHMs_pd.append(FWHMs[i])\n",
    "\n",
    "                        # apply bad pixel mask (same for both PyTorchDIA and pyDANDIA)\n",
    "                        masked_model_pd = np.ma.array(model, mask=mask)\n",
    "                        masked_image_stamp = np.ma.array(image_stamp, mask=mask)\n",
    "                        masked_pixel_uncertanties = np.ma.array(pixel_uncertanties, mask=mask)\n",
    "\n",
    "                        # metrics\n",
    "                        MFB_pd, MFV_pd = metrics(masked_model_pd, masked_image_stamp,\n",
    "                                                 masked_pixel_uncertanties, ks, mask)\n",
    "                        \n",
    "                        print('P:', np.sum(kernel_pd))\n",
    "                        print('B0:', B0_pd)\n",
    "                        print('MFB:', MFB_pd)\n",
    "                        print('MFV:', MFV_pd)\n",
    "\n",
    "                        B0s_pd.append(B0_pd)\n",
    "                        MFBs_pd.append(MFB_pd)\n",
    "                        MFVs_pd.append(MFV_pd)\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        ## Measured flux of brightest star\n",
    "                        F_measured = F_diff / np.sum(kernel_pt)\n",
    "\n",
    "                        # store values to cal\n",
    "                        F_measured_values_pt.append(F_measured)\n",
    "                        photometric_scale_factors_pt.append(np.sum(kernel_pt))\n",
    "\n",
    "                        # stamp number\n",
    "                        stamp_ids_pt.append(j)\n",
    "\n",
    "                        # image SNR\n",
    "                        sky = np.median(image_stamp)\n",
    "                        SNR = np.sum(image_stamp - sky) / np.sqrt(np.sum(pixel_uncertanties**2))\n",
    "                        image_SNRs_pt.append(SNR)\n",
    "                        image_FWHMs_pt.append(FWHMs[i])\n",
    "\n",
    "                        # apply bad pixel mask (same for both PyTorchDIA and pyDANDIA)\n",
    "                        masked_model_pt = np.ma.array(model, mask=mask)\n",
    "                        masked_image_stamp = np.ma.array(image_stamp, mask=mask)\n",
    "                        masked_pixel_uncertanties = np.ma.array(pixel_uncertanties, mask=mask)\n",
    "\n",
    "                        # metrics\n",
    "                        MFB_pt, MFV_pt = metrics(masked_model_pt, masked_image_stamp,\n",
    "                                                 masked_pixel_uncertanties, ks, mask)\n",
    "                        \n",
    "                        print('P:', np.sum(kernel_pt))\n",
    "                        print('B0:', B0_pt)\n",
    "                        print('MFB:', MFB_pt)\n",
    "                        print('MFV:', MFV_pt)\n",
    "\n",
    "                        B0s_pt.append(B0_pt)\n",
    "                        MFBs_pt.append(MFB_pt)\n",
    "                        MFVs_pt.append(MFV_pt)\n",
    "\n",
    "            except (ValueError, np.linalg.LinAlgError) as e:\n",
    "                print('Skipping reference-image pair... target too close to border for this imge FWHM')\n",
    "\n",
    "\n",
    "        # as with 'true' pixel uncertanties, use pyDANDIA to calculate\n",
    "        # 'true' photometric scale factor\n",
    "        P_true = np.median(photometric_scale_factors_pd)\n",
    "\n",
    "        print('\\npyDANDIA photometric residuals')\n",
    "        for s in range(0, len(photometric_scale_factors_pd)):\n",
    "            P_normalised = photometric_scale_factors_pd[s]/P_true\n",
    "            Ps_normalised_pd.append(P_normalised)\n",
    "            min_var = (1./P_true**2) * (np.sum((target_psf_objects[s]**2/pixel_uncertanties_list[s]**2)))**(-1)\n",
    "            F_measured_sigma_min = F_measured_values_pd[s] / np.sqrt(min_var)\n",
    "            norm_phot_resids_pd.append(F_measured_sigma_min)\n",
    "            print('F_measure / sigma_min:', F_measured_sigma_min)\n",
    "\n",
    "        # PyTorch - calculate theoretical min_var for each stamp\n",
    "        print('\\nPyTorchDIA photometric residuals')\n",
    "        for s in range(0, len(photometric_scale_factors_pt)):\n",
    "            P_normalised = photometric_scale_factors_pt[s]/P_true\n",
    "            Ps_normalised_pt.append(P_normalised)\n",
    "            min_var = (1./P_true**2) * (np.sum((target_psf_objects[s]**2/pixel_uncertanties_list[s]**2)))**(-1)\n",
    "            F_measured_sigma_min = F_measured_values_pt[s] / np.sqrt(min_var)\n",
    "            norm_phot_resids_pt.append(F_measured_sigma_min)\n",
    "            print('F_measure / sigma_min:', F_measured_sigma_min)\n",
    "\n",
    "\n",
    "\n",
    "        out_pd = np.vstack((MFBs_pd, MFVs_pd, Ps_normalised_pd, np.array(norm_phot_resids_pd).flatten(),\n",
    "                         image_SNRs_pd, image_FWHMs_pd, stamp_ids_pd, ll_pds)).T\n",
    "\n",
    "        out_pt = np.vstack((MFBs_pt, MFVs_pt, Ps_normalised_pt, np.array(norm_phot_resids_pt).flatten(),\n",
    "                         image_SNRs_pt, image_FWHMs_pt, stamp_ids_pt, ll_pts)).T\n",
    "\n",
    "        \n",
    "        \n",
    "        #path = '/media/james/Seagate_Expansion_Drive#2'\n",
    "        path = os.getcwd()\n",
    "\n",
    "        filename = os.path.join(path, 'pyDANDIA_OGLE_BLG101_December2020_TestRun.txt')\n",
    "        with open(filename, 'a') as f:\n",
    "            np.savetxt(f, out_pd)\n",
    "        \n",
    "        filename = os.path.join(path, 'PyTorchDIA_OGLE_BLG101_December_2020_TestRun.txt')\n",
    "        with open(filename, 'a') as f:\n",
    "            np.savetxt(f, out_pt)\n",
    "        \n",
    "\n",
    "print(len(MFBs), len(MFVs), len(Ps_normalised), len(norm_phot_resids))\n",
    "print('Total time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Real Image Performance Tests",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
